import streamlit as st
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from transformers.trainer_callback import TrainerCallback
from datasets import Dataset
import torch
import os
import json
import pandas as pd

st.set_page_config(page_title="Ph√¢n T√≠ch C·∫£m X√∫c", layout="wide")

MODEL_DIR = "models"
HISTORY_PATH = "history.json"

@st.cache_resource
def load_model(model_path):
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForSequenceClassification.from_pretrained(model_path)
    return tokenizer, model

def preprocess_data(df, tokenizer):
    df['text'] = df['text'].astype(str)
    dataset = Dataset.from_pandas(df)
    def preprocess(examples):
        return tokenizer(examples['text'], truncation=True, padding=True, max_length=258)
    return dataset.map(preprocess, batched=True)

def get_next_model_dir(base_dir=MODEL_DIR):
    if not os.path.exists(base_dir):
        os.makedirs(base_dir)
        return os.path.join(base_dir, "model_1")
    existing = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d)) and d.startswith("model_")]
    indices = [int(d.split("_")[1]) for d in existing if d.split("_")[1].isdigit()]
    next_index = max(indices) + 1 if indices else 1
    return os.path.join(base_dir, f"model_{next_index}")

# Callback ƒë·ªÉ log qu√° tr√¨nh training
class StreamlitCallback(TrainerCallback):
    def __init__(self):
        super().__init__()
        self.logs = []

    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs is not None:
            self.logs.append(logs)
            if 'loss' in logs:
                st.session_state['training_logs'].append(f"Epoch {int(state.epoch)} - Loss: {logs['loss']}")

def train_and_save_model(uploaded_file, num_train_epochs=2):
    if uploaded_file is None:
        st.warning("Vui l√≤ng upload file d·ªØ li·ªáu")
        return False

    try:
        # ƒê·ªçc file CSV ho·∫∑c JSON
        if uploaded_file.name.endswith('.csv'):
            df = pd.read_csv(uploaded_file)
        elif uploaded_file.name.endswith('.json'):
            df = pd.read_json(uploaded_file)
        else:
            st.error("Ch·ªâ h·ªó tr·ª£ ƒë·ªãnh d·∫°ng CSV ho·∫∑c JSON")
            return False
        
        if 'text' not in df.columns or 'label' not in df.columns:
            st.error("File ph·∫£i c√≥ c·ªôt 'text' v√† 'label'")
            return False
        
        # Hi·ªÉn th·ªã b·∫£ng d·ªØ li·ªáu v·ª´a upload
        st.subheader("üìä D·ªØ li·ªáu hu·∫•n luy·ªán:")
        st.dataframe(df)

        output_dir = get_next_model_dir()
        os.makedirs(output_dir, exist_ok=True)

        pretrained_model_name = "roberta-base"

        tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)
        model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name, num_labels=3)
        tokenized = preprocess_data(df, tokenizer)

        args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=num_train_epochs,
            per_device_train_batch_size=8,
            save_steps=500,
            save_total_limit=1,
            logging_dir=f"{output_dir}/logs",
            logging_steps=100,
            remove_unused_columns=False,
        )

        # Kh·ªüi t·∫°o session_state ƒë·ªÉ l∆∞u log training
        if 'training_logs' not in st.session_state:
            st.session_state['training_logs'] = []

        trainer = Trainer(
            model=model, 
            args=args, 
            train_dataset=tokenized, 
            callbacks=[StreamlitCallback()]
        )

        with st.spinner("ƒêang hu·∫•n luy·ªán model..."):
            trainer.train()

        model.save_pretrained(output_dir)
        tokenizer.save_pretrained(output_dir)
        return True
    except Exception as e:
        st.error(f"L·ªói khi x·ª≠ l√Ω file: {e}")
        return False

def save_history(text, label):
    if os.path.exists(HISTORY_PATH):
        with open(HISTORY_PATH, "r", encoding="utf-8") as f:
            history = json.load(f)
    else:
        history = []
    history.append({"text": text, "label": label})
    with open(HISTORY_PATH, "w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=2)

def load_history():
    if os.path.exists(HISTORY_PATH):
        with open(HISTORY_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

# Sidebar
st.sidebar.header("Ch·ª©c nƒÉng")

if os.path.exists(MODEL_DIR):
    model_list = [os.path.join(MODEL_DIR, d) for d in sorted(os.listdir(MODEL_DIR)) if os.path.isdir(os.path.join(MODEL_DIR, d))]
else:
    model_list = []

if not model_list:
    st.sidebar.warning("Ch∆∞a c√≥ model n√†o trong th∆∞ m·ª•c models. Vui l√≤ng train model m·ªõi.")
else:
    selected_model = st.sidebar.selectbox("Ch·ªçn model ƒë·ªÉ d·ª± ƒëo√°n", model_list)

uploaded_file = st.sidebar.file_uploader("Upload CSV ho·∫∑c JSON ch·ª©a c·ªôt 'text' v√† 'label' ƒë·ªÉ hu·∫•n luy·ªán", type=['csv', 'json'])

if st.sidebar.button("Hu·∫•n luy·ªán model"):
    success = train_and_save_model(uploaded_file)
    if success:
        st.success("‚úÖ Hu·∫•n luy·ªán th√†nh c√¥ng! Model ƒë√£ l∆∞u v√† s·∫µn s√†ng d·ª± ƒëo√°n.")
        # Reload model list sau khi train xong
        model_list = [os.path.join(MODEL_DIR, d) for d in sorted(os.listdir(MODEL_DIR)) if os.path.isdir(os.path.join(MODEL_DIR, d))]
        selected_model = model_list[-1]
        # Hi·ªÉn th·ªã log training
        st.subheader("üìà Log qu√° tr√¨nh hu·∫•n luy·ªán:")
        for log in st.session_state['training_logs']:
            st.write(log)
    else:
        st.error("‚ùå Hu·∫•n luy·ªán th·∫•t b·∫°i!")

if model_list:
    tokenizer, model = load_model(selected_model)
else:
    st.warning("Kh√¥ng c√≥ model ƒë·ªÉ load. Vui l√≤ng train model tr∆∞·ªõc.")
    st.stop()

# Ph·∫ßn d·ª± ƒëo√°n
st.title("üîç Ph√¢n T√≠ch C·∫£m X√∫c VƒÉn B·∫£n")

input_text = st.text_area("Nh·∫≠p vƒÉn b·∫£n ƒë·ªÉ d·ª± ƒëo√°n c·∫£m x√∫c:", height=150)

if st.button("D·ª± ƒëo√°n"):
    if input_text.strip():
        with st.spinner("ƒêang d·ª± ƒëo√°n..."):
            inputs = tokenizer(input_text, return_tensors="pt", truncation=True, padding=True)
            with torch.no_grad():
                outputs = model(**inputs)
                logits = outputs.logits
                predicted_class = torch.argmax(logits, dim=1).item()
            label_map = {0: "Ti√™u c·ª±c", 1: "Trung t√≠nh", 2: "T√≠ch c·ª±c"}
            predicted_label = label_map.get(predicted_class, "Kh√¥ng x√°c ƒë·ªãnh")
            save_history(input_text, predicted_label)
            st.success(f"K·∫øt qu·∫£ d·ª± ƒëo√°n: **{predicted_label}**")
        history = load_history()
        st.subheader("üïò L·ªãch s·ª≠ d·ª± ƒëo√°n (t·ªëi ƒëa 10):")
        for i, record in enumerate(reversed(history[-10:]), 1):
            st.write(f"{i}. [{record['label']}] {record['text']}")
    else:
        st.warning("Vui l√≤ng nh·∫≠p vƒÉn b·∫£n ƒë·ªÉ d·ª± ƒëo√°n.")
